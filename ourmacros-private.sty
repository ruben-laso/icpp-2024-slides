
\newcommand{\List}{Listing\xspace}
\newcommand{\Lists}{Listings\xspace}
\newcommand{\fig}{Figure\xspace}
\newcommand{\Fig}{Fig.\xspace}
\newcommand{\figs}{Figures\xspace}
\newcommand{\tab}{Table\xspace}
\newcommand{\Tab}{Tab.\xspace}
\newcommand{\alg}{Algorithm\xspace}
\newcommand{\algs}{Algorithms\xspace}
\newcommand{\append}{Appendix\xspace}
\newcommand{\Sec}{Section\xspace}
\newcommand{\Secs}{Sections\xspace}
\newcommand{\App}{Appendix\xspace}
\newcommand{\Sect}{Section\xspace}
\newcommand{\Equ}{Equation\xspace}
\newcommand{\Equs}{Equations\xspace}
\newcommand{\Line}{line\xspace}
\newcommand{\eg}{e.g.\xspace}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\etcet}{etc.\xspace}
\newcommand{\cf}{cf.\xspace}

\newcommand{\Runtime}{Run-time\xspace}
\newcommand{\runtime}{run-time\xspace}
\newcommand{\runtimes}{run-times\xspace}

\newcommand{\Bytes}{Bytes\xspace}
\newcommand{\Byte}{Byte\xspace}

% \newcommand{\chartcaptionproblemsize}{Execution time scalability with the problem size. All cores were used except for GCC’s sequential implementation\xspace}
\newcommand{\chartcaptionproblemsize}{Problem scaling using all cores except for GCC’s seq. implementation\xspace}
\newcommand{\chartcaptionnumcores}{Execution time scalability with the number of threads used. Problem size is $2^{30}$\xspace}
\newcommand{\chartcaptionspeedup}{Strong scaling with $2^{30}$ elements\xspace}

%\newcommand{\jupitermvapich}{\productname{MVAPICH2-2.2}\xspace}
\newcommand{\jupiteropenmpi}{\productname{Open\,MPI~4.0.3}\xspace}
\newcommand{\supermucopenmpi}{\productname{Open\,MPI~4.0.2}\xspace}
\newcommand{\hydraopenmpi}{\productname{Open\,MPI~3.1.3}\xspace}
\newcommand{\hydraintelmpi}{\productname{Intel\,MPI~2019}\xspace}
%\newcommand{\jupiteropenmpilatest}{\productname{Open\,MPI~2.1.0}\xspace}
%\newcommand{\jupiteropenmpilatestversion}{\productname{2.1.0}\xspace}

\newcommand{\gcchydra}{gcc~12.1.0\xspace}
\newcommand{\gpphydra}{g++~12.1.0\xspace}
\newcommand{\icpxhydra}{icpx~2021.7.0\xspace}
\newcommand{\clanghydra}{clang~14.0.4\xspace}
\newcommand{\pgcchydra}{pgcc~22.5\xspace}
\newcommand{\nvhpchydra}{nvc++~22.11\xspace}

\newcommand{\gccnebula}{gcc~12.3.0\xspace}
\newcommand{\gppnebula}{g++~12.3.0\xspace}
\newcommand{\nvhpcnebula}{nvc++~23.7\xspace}

\newcommand{\gccvsc}{gcc~12.2.0\xspace}
\newcommand{\gppvsc}{g++~12.2.0\xspace}
\newcommand{\icpxvsc}{icpx~2022.2.1\xspace}
\newcommand{\nvhpcvsc}{nvc++~22.9\xspace}

\newcommand{\gcctesla}{gcc~10.2.1\xspace}
\newcommand{\gpptesla}{g++~10.2.1\xspace}
\newcommand{\nvhpctesla}{nvc++~23.5\xspace}

\newcommand{\gccampere}{gcc~10.2.1\xspace}
\newcommand{\gppampere}{g++~10.2.1\xspace}
\newcommand{\nvhpcampere}{nvc++~23.5\xspace}

\newcommand{\tbbhydra}{TBB~2021.9.0\xspace}
\newcommand{\hpxhydra}{HPX~1.9.0\xspace}
\newcommand{\gomphydra}{GOMP~(g++)\xspace}
\newcommand{\nvomphydra}{NVOMP~22.11\xspace}

\newcommand{\tbbnebula}{TBB~2021.10.0\xspace}
\newcommand{\hpxnebula}{HPX~1.9.1\xspace}
\newcommand{\gompnebula}{GOMP~(g++)\xspace}
\newcommand{\nvompnebula}{NVOMP~22.11\xspace}

\newcommand{\tbbvsc}{TBB~2021.7.0\xspace}
\newcommand{\hpxvsc}{HPX~1.8.1\xspace}
\newcommand{\gompvsc}{GOMP~(g++)\xspace}
\newcommand{\nvompvsc}{NVOMP~22.9\xspace}

\newcommand{\cudatesla}{CUDA~11.8\xspace}
\newcommand{\cudaampere}{CUDA~12.2\xspace}

\newcommand{\nvidia}{NVIDIA\xspace}

\newcommand{\juliapaperversion}{1.4.0}
\newcommand{\juliahydra}{Julia \juliapaperversion\xspace}
\newcommand{\julianebula}{Julia \juliapaperversion\xspace}

\newcommand{\machirene}{\mbox{\emph{Joliot-Curie Rome}}\xspace}
\newcommand{\machtheta}{\mbox{\emph{Theta}}\xspace}
\newcommand{\machlumi}{\mbox{\emph{LUMI}}\xspace}
\newcommand{\machdiscoverer}{\mbox{\emph{Discoverer}}\xspace}

\newcommand{\machhydra}{\emph{Hydra}\xspace}
\newcommand{\machnebula}{\emph{Nebula}\xspace}
\newcommand{\machvsc}{\mbox{\emph{VSC-5}}\xspace}
\newcommand{\machtesla}{\mbox{\emph{Tesla}}\xspace}
\newcommand{\machampere}{\mbox{\emph{Ampere}}\xspace}
\newcommand{\machhydralong}{\textbf{Hydra (Skylake)}\xspace}
\newcommand{\machnebulalong}{\textbf{Nebula (Zen~1)}\xspace}
\newcommand{\machvsclong}{\mbox{\textbf{VSC-5 (Zen~3)}}\xspace}
\newcommand{\machteslalong}{\mbox{\textbf{Tesla}}\xspace}
\newcommand{\machamperelong}{\mbox{\textbf{Ampere}}\xspace}

% \newcommand{\machhydra}{\emph{Mach~A}\xspace}
% \newcommand{\machnebula}{\emph{Mach~B}\xspace}
% \newcommand{\machvsc}{\mbox{\emph{Mach~C}}\xspace}
% \newcommand{\machtesla}{\mbox{\emph{Mach~D}}\xspace}
% \newcommand{\machampere}{\mbox{\emph{Mach~E}}\xspace}
% \newcommand{\machhydralong}{\emph{Mach~A} (Skylake)\xspace}
% \newcommand{\machnebulalong}{\emph{Mach~B} (Zen~1)\xspace}
% \newcommand{\machvsclong}{\mbox{\emph{Mach~C}} (Zen~3)\xspace}
% \newcommand{\machteslalong}{\mbox{\emph{Mach~D}} (Tesla)\xspace}
% \newcommand{\machamperelong}{\mbox{\emph{Mach~E}} (Ampere)\xspace}

\newcommand{\machhydrampiversionopenmpi}{\mbox{Open\,MPI 4.1.4}\xspace}
\newcommand{\machhydrampiversionmvapich}{\mbox{MVAPICH2 2.3.7}\xspace}
\newcommand{\machhydrampiversionintelmpi}{\mbox{Intel\,MPI 2021.7}\xspace}
\newcommand{\machirenempiversion}{\mbox{Open\,MPI 4.1.4}\xspace}
%\newcommand{\machthetampiversion}{\mbox{Intel\,MPI 19.1.0.166}\xspace}
\newcommand{\machthetampiversion}{\mbox{Cray MPICH 7.7.14}\xspace}
\newcommand{\machdiscoverermpiversion}{\mbox{Open\,MPI 4.1.4}\xspace}

\newcommand{\mpiprocnull}{\texttt{MPI\_PROC\_NULL}\xspace}
\newcommand{\mpicommworld}{\texttt{MPI\_COMM\_WORLD}\xspace}
\newcommand{\mpicommtypeshared}{\texttt{MPI\_COMM\_TYPE\_SHARED}\xspace}
\newcommand{\mpiinplace}{\texttt{MPI\_IN\_PLACE}\xspace}

\newcommand{\mpiinfo}{\texttt{MPI\_Info}\xspace}

\newcommand{\mpicartrank}{\texttt{MPI\_Cart\_rank}\xspace}
\newcommand{\mpicartshift}{\texttt{MPI\_Cart\_shift}\xspace}

\newcommand{\mpidimscreate}{\texttt{MPI\_Dims\_create}\xspace}

\newcommand{\mpicartcreate}{\texttt{MPI\_Cart\_create}\xspace}
\newcommand{\mpidistgraphcreateadjacent}{\texttt{MPI\_Dist\_graph\_create\_adjacent}\xspace}
\newcommand{\mpidistgraphcreate}{\texttt{MPI\_Dist\_graph\_create}\xspace}

\newcommand{\mpineighboralltoall}{\texttt{MPI\_Neighbor\_alltoall}\xspace}
\newcommand{\mpineighboralltoallv}{\texttt{MPI\_Neighbor\_alltoallv}\xspace}
\newcommand{\mpineighboralltoallw}{\texttt{MPI\_Neighbor\_alltoallw}\xspace}

\newcommand{\mpiwtime}{\texttt{MPI\_Wtime}\xspace}

\newcommand{\mpibsend}{\texttt{MPI\_Bsend}\xspace}
\newcommand{\mpirsend}{\texttt{MPI\_Rsend}\xspace}
\newcommand{\mpisend}{\texttt{MPI\_Send}\xspace}
\newcommand{\mpirecv}{\texttt{MPI\_Recv}\xspace}
\newcommand{\mpisendrecv}{\texttt{MPI\_Sendrecv}\xspace}

\newcommand{\mpiisend}{\texttt{MPI\_Isend}\xspace}
\newcommand{\mpiirecv}{\texttt{MPI\_Irecv}\xspace}
\newcommand{\mpiibcast}{\texttt{MPI\_Ibcast}\xspace}

\newcommand{\mpiwinfence}{\texttt{MPI\_Win\_fence}\xspace}
\newcommand{\mpiput}{\texttt{MPI\_Put}\xspace}

\newcommand{\mpibarrier}{\texttt{MPI\_Barrier}\xspace}
\newcommand{\mpibcast}{\texttt{MPI\_Bcast}\xspace}
\newcommand{\mpibcastlane}{\texttt{MPI\_Bcast}_{\text{lane}}\xspace}
\newcommand{\mpibcasthier}{\texttt{MPI\_Bcast}_{\text{hier}}\xspace}

\newcommand{\mpiscatter}{\texttt{MPI\_Scatter}\xspace}
\newcommand{\mpiscatterv}{\texttt{MPI\_Scatterv}\xspace}
\newcommand{\mpigather}{\texttt{MPI\_Gather}\xspace}
\newcommand{\mpigatherv}{\texttt{MPI\_Gatherv}\xspace}
\newcommand{\mpiallgather}{\texttt{MPI\_Allgather}\xspace}
\newcommand{\mpiallgatherv}{\texttt{MPI\_Allgatherv}\xspace}
\newcommand{\mpialltoall}{\texttt{MPI\_Alltoall}\xspace}
\newcommand{\mpialltoallv}{\texttt{MPI\_Alltoallv}\xspace}
\newcommand{\mpialltoallw}{\texttt{MPI\_Alltoallw}\xspace}

\newcommand{\mpireduce}{\texttt{MPI\_Reduce}\xspace}
\newcommand{\mpiallreduce}{\texttt{MPI\_Allreduce}\xspace}
\newcommand{\mpireducescatter}{\texttt{MPI\_Reduce\_scatter}\xspace}
\newcommand{\mpireducescatterblock}{\texttt{MPI\_Reduce\_scatter\_block}\xspace}
\newcommand{\mpiscan}{\texttt{MPI\_Scan}\xspace}
\newcommand{\mpiexscan}{\texttt{MPI\_Exscan}\xspace}
\newcommand{\mpireducelocal}{\texttt{MPI\_Reduce\_local}\xspace}

\newcommand{\mpitypeindexed}{\texttt{MPI\_Type\_indexed}\xspace}
%\newcommand{\mpiinplace}{\texttt{MPI\_IN\_PLACE}\xspace}


%\newcommand{\mpirun}{\texttt{mpirun}\xspace}
%\newcommand{\mpicc}{\texttt{mpicc}\xspace}

\newcommand{\mpiinit}{\texttt{MPI\_Init}\xspace}
\newcommand{\mpifinalize}{\texttt{MPI\_Finalize}\xspace}

\newcommand{\mpiwait}{\texttt{MPI\_Wait}\xspace}

\newcommand{\mpiint}{\texttt{MPI\_INT}\xspace}
\newcommand{\mpibor}{\texttt{MPI\_BOR}\xspace}


\newcommand{\productname}[1]{#1} %{\textsf{#1}}

\newcommand{\skampi}{SKaMPI\xspace}
\newcommand{\reprompi}{ReproMPI\xspace}
\newcommand{\reprompiurl}{https://github.com/hunsa/reprompi}
\newcommand{\mpptest}{MPPTEST\xspace}
\newcommand{\netgauge}{Netgauge\xspace}
\newcommand{\osu}{OSU Micro-Benchmarks\xspace}
\newcommand{\imb}{Intel MPI Benchmarks\xspace}
\newcommand{\mpibench}{MPIBench\xspace}
\newcommand{\mpicroscope}{mpicroscope\xspace}
\newcommand{\mpicroscopekernel}{mpicroscope\_kernel\xspace}
\newcommand{\openmpi}{Open\,MPI\xspace}
\newcommand{\necmpi}{NEC\,MPI\xspace}
\newcommand{\intelmpi}{Intel\,MPI\xspace}
\newcommand{\intelmpitune}{\texttt{mpitune}\xspace}
\newcommand{\otpo}{\texttt{otpo}}
\newcommand{\mvapich}{MVAPICH\xspace}
\newcommand{\mpich}{MPICH\xspace}
\newcommand{\infiniband}{InfiniBand\xspace}
\newcommand{\opa}{Intel OmniPath\xspace}
\newcommand{\gcc}{gcc\xspace}
\newcommand{\GCC}{GCC\xspace}
\newcommand{\clang}{clang\xspace}
\newcommand{\pgcc}{pgcc\xspace}
\newcommand{\icc}{icc\xspace}
\newcommand{\mpirun}{\texttt{mpirun}\xspace}
\newcommand{\mpiruns}{\texttt{mpirun}s\xspace}
\newcommand{\hwloc}{\texttt{hwloc}\xspace}
\newcommand{\openmp}{OpenMP\xspace}

\newcommand{\nbcbench}{{NBCBench}\xspace}
\newcommand{\phloem}{{Phloem}\xspace}
\newcommand{\OSU}{{OSU Micro-Benchmarks}\xspace}
\newcommand{\osugather}{\texttt{osu\_gather}\xspace}

\newcommand{\opentuner}{OpenTuner\xspace}


\newcommand{\stream}{STREAM\xspace}
\newcommand{\linpack}{LINPACK\xspace}

\newcommand{\streamjl}{STREAM.jl\xspace}
\newcommand{\reprompijl}{ReproMPI.jl\xspace}
\newcommand{\mpijl}{MPI.jl\xspace}
\newcommand{\reprompilib}{ReproMPIlib\xspace}

\newcommand{\mpip}{mpiP\xspace}
\newcommand{\scorep}{Score-P\xspace}
\newcommand{\pilgrim}{Pilgrim\xspace}
\newcommand{\scalatrace}{ScalaTrace\xspace}
\newcommand{\likwid}{Likwid\xspace}
\newcommand{\papi}{PAPI\xspace}
\newcommand{\hpctoolkit}{HPCToolkit\xspace}
\newcommand{\caliper}{Caliper\xspace}
\newcommand{\paraver}{Paraver\xspace}
\newcommand{\ipm}{IPM\xspace}
\newcommand{\vampir}{Vampir\xspace}
\newcommand{\tautool}{Tau\xspace}
\newcommand{\scalasca}{Scalasca\xspace}
\newcommand{\extrae}{Extrae\xspace}

\newcommand{\ecpamg}{AMG\xspace}
\newcommand{\ecpexaminimd}{ExaMiniMD\xspace}
\newcommand{\ecpminiamr}{miniAMR\xspace}
\newcommand{\ecpminivite}{miniVite\xspace}
\newcommand{\ecpswlite}{SW4Lite\xspace}

\newcommand{\atompicolltune}{OMPICollTune\xspace}
\newcommand{\pgchecker}{\texttt{MPINPC}\xspace}
\newcommand{\pgcheckerexpl}{\texttt{MPINPC is Not a Performance Checker}\xspace}


\newcommand{\pstlbench}{\emph{pSTL-Bench}\xspace}
\newcommand{\langcpp}{C\texttt{++}\xspace}
\newcommand{\langcppsvtn}{C\texttt{++}17\xspace}

\newcommand{\groupfind}{\texttt{find}\xspace}
\newcommand{\groupreduce}{\texttt{reduce}\xspace}
\newcommand{\groupforeach}{\texttt{for\_each}\xspace}
\newcommand{\groupsort}{\texttt{sort}\xspace}
\newcommand{\groupinclscan}{\texttt{inclusive\_scan}\xspace}
\newcommand{\groupscan}{\texttt{*clusive\_scan}\xspace}

\newcommand{\higherisbetter}{Higher is better\xspace}
\newcommand{\lowerisbetter}{Lower is better\xspace}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "pstl_bench_main"
%%% End:
